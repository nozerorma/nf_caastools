#!/usr/bin/env nextflow

/*
#
#
#  ██████╗ ██╗  ██╗██╗   ██╗██╗      ██████╗ ██████╗ ██╗  ██╗███████╗██████╗ ███████╗
#  ██╔══██╗██║  ██║╚██╗ ██╔╝██║     ██╔═══██╗██╔══██╗██║  ██║██╔════╝██╔══██╗██╔════╝
#  ██████╔╝███████║ ╚████╔╝ ██║     ██║   ██║██████╔╝███████║█████╗  ██████╔╝█████╗  
#  ██╔═══╝ ██╔══██║  ╚██╔╝  ██║     ██║   ██║██╔═══╝ ██╔══██║██╔══╝  ██╔══██╗██╔══╝  
#  ██║     ██║  ██║   ██║   ███████╗╚██████╔╝██║     ██║  ██║███████╗██║  ██║███████╗
#  ╚═╝     ╚═╝  ╚═╝   ╚═╝   ╚══════╝ ╚═════╝ ╚═╝     ╚═╝  ╚═╝╚══════╝╚═╝  ╚═╝╚══════╝
#                                                                                    
#                                      
# PHYLOPHERE: A Nextflow pipeline including a complete set
# of phylogenetic comparative tools and analyses for Phenome-Genome studies
#
# Github: https://github.com/nozerorma/caastools/nf-phylophere
#
# Author:         Miguel Ramon (miguel.ramon@upf.edu)
#
# File: nextflow.config.bak
#
*/

/*
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 * Configuration file for setting up global parameters, process-specific resource requirements, 
 * and execution profiles. This ensures optimal resource allocation and flexibility across 
 * different compute environments.
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 */

process.container = 'docker.io/miralnso/caastools-micromamba:latest'

// Redirect logs to stable log dir: $ export NXF_LOG_FILE="log/nextflow.log" (must be set in console before running the script)

// Global default params, used in configs
params {

    // CT general option
    //ct_title                    = ct_title                  ?:  null
    //ct_logo                     = ct_logo                   ?:  null
    ct_tool                     = ct_tool                   ?:  "discovery,resample,bootstrap"

    // Common DISCOVERY and BOOTSTRAP options
    
    alignment                   = alignment                 ?:  "$baseDir/examples/MSA"
    disc_description            = disc_description          ?:  null
    traitfile                   = traitfile                 ?:  "$baseDir/examples/config.tab"
    ali_format                  = ali_format                ?:  "phylip-relaxed" // would be great if this could be parsed from input
    patterns                    = patterns                  ?:  "1,2,3"
    maxbggaps                   = maxbggaps                 ?:  "NO"  
    maxfggaps                   = maxfggaps                 ?:  "NO"
    maxgaps                     = maxgaps                   ?:  "NO"
    maxgapsperposition          = maxgapsperposition        ?:  "0.5"
    maxbgmiss                   = maxbgmiss                 ?:  "NO"
    maxfgmiss                   = maxfgmiss                 ?:  "NO"
    maxmiss                     = maxmiss                   ?:  "NO"

    // RESAMPLE options
    tree                       = tree                       ?:  "$baseDir/examples/phylogeny.nw"
    strategy                   = strategy                   ?:  "BM"
    fgsize                     = fgsize                     ?:  "5"
    bgsize                     = bgsize                     ?:  "4"
    template                   = template                   ?:  "$baseDir/examples/config.tab"
    bygroup                    = bygroup                    ?:  "$baseDir/test/sp2fam.210727.tab"
    traitvalues                = traitvalues                ?:  "$baseDir/examples/traitvalues.tab"
    cycles                     = cycles                     ?:  "1000"
    
    // BOOTSTRAP options
    resample_out               = resample_out               ?:  null    

    // Boilerplate options
    outdir                     = outdir                     ?: "$baseDir/results" 
    //publish_dir_mode           = publish_dir_mode           ?: 'copy' // redundant
    //email                      = null
    //email_on_fail              = null
    //plaintext_email            = false
    monochrome_logs            = false
    hook_url                   = null
    help                       = false
    version                    = false

/* 
    // Config options
    config_profile_name        = null
    config_profile_description = null
    custom_config_version      = 'master'
    custom_config_base         = "https://raw.githubusercontent.com/nf-core/configs/${params.custom_config_version}"
    config_profile_contact     = null
    config_profile_url         = null */


    // Max resource options
    // Defaults only, expecting to be overwritten
    max_memory                 = '128.GB'
    max_cpus                   = 16
    max_time                   = '240.h'

/*
    // Schema validation default options
    validationFailUnrecognisedParams = false
    validationLenientMode            = false
    validationSchemaIgnoreParams     = 'genomes'
    validationShowHiddenParams       = false
    validate_params                  = true */

}

// Load base.config by default for all pipelines
includeConfig 'conf/base.config'

// Load nf-core/testpipeline custom profiles from different institutions.
// Warning: Uncomment only if a pipeline-specific instititutional config already exists on nf-core/configs!
// try {
//   includeConfig "${params.custom_config_base}/pipeline/testpipeline.config"
// } catch (Exception e) {
//   System.err.println("WARNING: Could not load nf-core/config/testpipeline profiles: ${params.custom_config_base}/pipeline/testpipeline.config")
// }
profiles {
    debug {
        dumpHashes             = true
        process.beforeScript   = 'echo $HOSTNAME'
        cleanup                = false
    }
    conda {
        conda.enabled          = true
        conda.useMicromamba    = true
        docker.enabled         = false
        singularity.enabled    = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false
        apptainer.enabled      = false
    }
    mamba {
        conda.enabled          = true
        conda.useMamba         = true
        conda.useMicromamba    = true
        docker.enabled         = false
        singularity.enabled    = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false
        apptainer.enabled      = false
    }
    docker {
        docker.enabled         = true
        docker.userEmulation   = true
        conda.enabled          = false
        singularity.enabled    = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false
        apptainer.enabled      = false
    }
    arm {
        docker.runOptions = '-u $(id -u):$(id -g) --platform=linux/amd64'
    }
    singularity {
        singularity.enabled    = true
        singularity.autoMounts = true
        singularity.cacheDir   = "$baseDir/singularity"
        conda.enabled          = false
        docker.enabled         = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false
        apptainer.enabled      = true
    }
    podman {
        podman.enabled         = true
        conda.enabled          = false
        docker.enabled         = false
        singularity.enabled    = false
        shifter.enabled        = false
        charliecloud.enabled   = false
        apptainer.enabled      = false
    }
    shifter {
        shifter.enabled        = true
        conda.enabled          = false
        docker.enabled         = false
        singularity.enabled    = false
        podman.enabled         = false
        charliecloud.enabled   = false
        apptainer.enabled      = false
    }
    charliecloud {
        charliecloud.enabled   = true
        conda.enabled          = false
        docker.enabled         = false
        singularity.enabled    = false
        podman.enabled         = false
        shifter.enabled        = false
        apptainer.enabled      = false
    }
    apptainer {
        apptainer.enabled      = true
        apptainer.cacheDir     = "$baseDir/singularity"
        conda.enabled          = false
        docker.enabled         = false
        singularity.enabled    = true
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false
    }
    gitpod {
        executor.name          = 'local'
        executor.cpus          = 16
        executor.memory        = 60.GB
    }
    test      { includeConfig 'conf/test.config'      }
    test_full { includeConfig 'conf/test_full.config' }
}

/* // Nextflow plugins
plugins {
    id 'nf-validation' // Validation of pipeline parameters and creation of an input channel from a sample sheet
}
 */
// Export these variables to prevent local Python/R libraries from conflicting with those in the container
// The JULIA depot path has been adjusted to a fixed path `/usr/local/share/julia` that needs to be used for packages in the container.
// See https://apeltzer.github.io/post/03-julia-lang-nextflow/ for details on that. Once we have a common agreement on where to keep Julia packages, this is adjustable.

env {
    PYTHONNOUSERSITE = 1
    R_PROFILE_USER   = "/.Rprofile"
    R_ENVIRON_USER   = "/.Renviron"
    JULIA_DEPOT_PATH = "/usr/local/share/julia"
}

// Capture exit codes from upstream processes when piping
process.shell = ['/bin/bash', '-euo', 'pipefail']

def trace_timestamp = new java.util.Date().format( 'yyyy-MM-dd_HH-mm-ss')

timeline {
    enabled = true
    file    = "${params.outdir}/pipeline_info/execution_timeline_${trace_timestamp}.html"
}
report {
    enabled = true
    file    = "${params.outdir}/pipeline_info/execution_report_${trace_timestamp}.html"
}
trace {
    enabled = true
    file    = "${params.outdir}/pipeline_info/execution_trace_${trace_timestamp}.txt"
}
dag {
    enabled = true
    file    = "${params.outdir}/pipeline_info/pipeline_dag_${trace_timestamp}.html"
}


manifest {
    name            = 'nf-caastools'
    author          = """Miguel Ramon Alonso"""
    homePage        = 'https://github.com/nozerorma/caastools'
    description     = """Nexflow pipeline for running CAAStools analyses"""
    mainScript      = 'main.nf'
    nextflowVersion = '!>=23.04.0'
    version         = '1.0'
    doi             = ''
}

// Function to ensure that resource requirements don't go beyond
// a maximum limit
def check_max(obj, type) {
    if (type == 'memory') {
        try {
            if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
                return params.max_memory as nextflow.util.MemoryUnit
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'time') {
        try {
            if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
                return params.max_time as nextflow.util.Duration
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'cpus') {
        try {
            return Math.min( obj, params.max_cpus as int )
        } catch (all) {
            println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
            return obj
        }
    }
}

// Load modules.config for DSL2 module specific options
includeConfig 'conf/modules.config'

profiles {
  standard {
     process {
        containerOptions = { workflow.containerEngine == "docker|singularity|podman"       ? '-u $(id -u):$(id -g)': null}
        executor="local"
        memory='0.6G'
        cpus='1'
        time='6h'

        withLabel: 'twocpus' {
            memory='8G'
            cpus='2'
        }
   	  }
   }
   cluster {
     process {
        containerOptions = { workflow.containerEngine == "docker|singularity|podman"       ? '-u $(id -u):$(id -g)': null}
        executor="SGE"
        queue = "smallcpus"

        memory='1G'
        cpus='1'
        time='6h'

        withLabel: 'twocpus' {
            queue = "bigcpus"
            memory='4G'
            cpus='2'
        }
      }
   }

   cloud {
    workDir = 's3://nf-class-bucket-XXX/work'
    aws.region = 'eu-central-1'
    aws.batch.cliPath = '/home/ec2-user/miniconda/bin/aws'

   process {
       executor = 'awsbatch'
       queue = 'spot'
       memory='1G'
       cpus='1'
       time='6h'

       withLabel: 'twocpus' {
           memory='2G'
           cpus='2'
       }
    }
  }
}